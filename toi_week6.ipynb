{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kseohyuz/Toi_CNN2/blob/main/toi_week6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델링 실습 : Retina**\n",
        "  1. 데이터 다운로드 (이전 실습 불러오기)\n",
        "  2. 데이터 분리 (이전 실습 불러오기)\n",
        "  3. 데이터셋 클래스 정의 (이전 실습 코드 응용)\n",
        "  4. 모델 불러오기\n",
        "  5. 전이 학습\n",
        "  6. 예측"
      ],
      "metadata": {
        "id": "Ji4nN_-shYFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 데이터 다운로드**"
      ],
      "metadata": {
        "id": "kGVSb3-gi8rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqYQyhXbhUjM",
        "outputId": "87c4311b-834d-4a5a-aec2-8e88b9bf3850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Tutorial-Book-Utils'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 45 (delta 18), reused 17 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 11.62 KiB | 5.81 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16Gzn1w38yZLJpzmzKcRIRveFeHZtCEr7\n",
            "From (redirected): https://drive.google.com/uc?id=16Gzn1w38yZLJpzmzKcRIRveFeHZtCEr7&confirm=t&uuid=ad5d1ce2-3090-4663-8565-f569bdae6807\n",
            "To: /content/Face Mask Detection.zip\n",
            "100% 417M/417M [00:16<00:00, 25.4MB/s]\n",
            "Face Mask Detection.zip download complete!\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
        "!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection\n",
        "!unzip -q Face\\ Mask\\ Detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 데이터 분리**"
      ],
      "metadata": {
        "id": "yOf7UAGmjaZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "print(len(os.listdir('annotations')))\n",
        "print(len(os.listdir('images')))\n",
        "\n",
        "!mkdir test_images\n",
        "!mkdir test_annotations\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "idx = random.sample(range(853), 170)\n",
        "\n",
        "for img in np.array(sorted(os.listdir('images')))[idx]:\n",
        "    shutil.move('images/'+img, 'test_images/'+img)\n",
        "\n",
        "for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n",
        "    shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n",
        "\n",
        "print(len(os.listdir('annotations')))\n",
        "print(len(os.listdir('images')))\n",
        "print(len(os.listdir('test_annotations')))\n",
        "print(len(os.listdir('test_images')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w676N3dkjYW4",
        "outputId": "6aeeb75a-f210-4a4b-f236-10856f37c5c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "853\n",
            "853\n",
            "683\n",
            "683\n",
            "170\n",
            "170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 데이터셋 클래스 정의**"
      ],
      "metadata": {
        "id": "cjmPu-Z4jvCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "def generate_box(obj):\n",
        "\n",
        "    xmin = float(obj.find('xmin').text)\n",
        "    ymin = float(obj.find('ymin').text)\n",
        "    xmax = float(obj.find('xmax').text)\n",
        "    ymax = float(obj.find('ymax').text)\n",
        "\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "def generate_label(obj):\n",
        "\n",
        "    if obj.find('name').text == \"with_mask\":\n",
        "\n",
        "        return 1\n",
        "\n",
        "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
        "\n",
        "        return 2\n",
        "\n",
        "    return 0\n",
        "\n",
        "def generate_target(file):\n",
        "    with open(file) as f:\n",
        "        data = f.read()\n",
        "        soup = BeautifulSoup(data, \"html.parser\")\n",
        "        objects = soup.find_all(\"object\")\n",
        "\n",
        "        num_objs = len(objects)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for i in objects:\n",
        "            boxes.append(generate_box(i))\n",
        "            labels.append(generate_label(i))\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        return target\n",
        "\n",
        "def plot_image_from_output(img, annotation):\n",
        "\n",
        "    img = img.cpu().permute(1,2,0)\n",
        "\n",
        "    rects = []\n",
        "\n",
        "    for idx in range(len(annotation[\"boxes\"])):\n",
        "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
        "\n",
        "        if annotation['labels'][idx] == 0 :\n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
        "\n",
        "        elif annotation['labels'][idx] == 1 :\n",
        "\n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
        "\n",
        "        else :\n",
        "\n",
        "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
        "\n",
        "        rects.append(rect)\n",
        "\n",
        "    return img, rects\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.path = path\n",
        "        self.imgs = list(sorted(os.listdir(self.path)))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_image = self.imgs[idx]\n",
        "        file_label = self.imgs[idx][:-3] + 'xml'\n",
        "        img_path = os.path.join(self.path, file_image)\n",
        "\n",
        "        if 'test' in self.path:\n",
        "            label_path = os.path.join(\"test_annotations/\", file_label)\n",
        "        else:\n",
        "            label_path = os.path.join(\"annotations/\", file_label)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        target = generate_target(label_path)\n",
        "\n",
        "        to_tensor = torchvision.transforms.ToTensor()\n",
        "\n",
        "        if self.transform:\n",
        "            img, transform_target = self.transform(np.array(img), np.array(target['boxes']))\n",
        "            target['boxes'] = torch.as_tensor(transform_target)\n",
        "\n",
        "        # tensor로 변경\n",
        "        img = to_tensor(img)\n",
        "\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "dataset = MaskDataset('images/')\n",
        "test_dataset = MaskDataset('test_images/')\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "PbnoXDkdjy0S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 모델 불러오기"
      ],
      "metadata": {
        "id": "_IMAsviqkXn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "7qHJ0KmClXRM",
        "outputId": "4051e7cc-0c17-49de-ee9b-433e6aab5ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fBrHIG_1lrpz",
        "outputId": "11ff03ab-8331-4064-dd79-ba69ae7f08aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.19.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 3, pretrained=False, pretrained_backbone = True)"
      ],
      "metadata": {
        "id": "qB63MVOLl09J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8417a5-c61e-4f9d-87c5-2fe6abccbfad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 127MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 전이 학습"
      ],
      "metadata": {
        "id": "xTG69yaxmMFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_epochs = 30\n",
        "retina.to(device)\n",
        "\n",
        "# parameters\n",
        "params = [p for p in retina.parameters() if p.requires_grad] # gradient calculation이 필요한 params만 추출\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "len_dataloader = len(data_loader)\n",
        "\n",
        "# epoch 당 약 4분 소요\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "    retina.train()\n",
        "\n",
        "    i = 0\n",
        "    epoch_loss = 0\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = retina(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses\n",
        "    print(epoch_loss, f'time: {time.time() - start}')"
      ],
      "metadata": {
        "id": "JVZ_pHOfmN81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}